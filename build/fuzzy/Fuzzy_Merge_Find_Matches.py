# --------------------------------------------------------------------------------------------------
# Fuzzy_Merge_Find_Matches
#
# All files in this folder (fuzzy) handle the probabilistic record linkage of observations in the 
# Morningstar holdings data for which we lack a CUSIP identifier to other observations for which we
# do have an identifier. This allows us to assign a CUSIP to the former records via internal
# cross-linkage.
#
# This distributed job retrieves the serialized trained linker and data shards generated by 
# Fuzzy_Merge_Train_Linker. It then computes the pairwise match probabilities and stores the 
# matched data to disk, using the trained logistic regression model for probabilistic record linkage.
# --------------------------------------------------------------------------------------------------
from __future__ import print_function

from util.serialization import safe_deserialize, retrieve_linker, parse_missing_fields, \
    df_to_dict, get_list_chunks, get_list_chunks_fixedsize
from dedupe.predicates import StringPredicate, commonSixGram
from dedupe.blocking import Blocker

from multiprocessing import Pool
from tqdm import *

import os
import argparse
import pandas as pd
import cloudpickle
import numpy as np
import gc
import getpass
import sys
import logging


# Prepass settings (i.e. manual blocking on field groups)
prepass_rounds = {

    "bonds": [
        ["maturitydate", "coupon", "iso_country_code", "currency_id", "mns_subclass"],
        ["maturitydate", "coupon", "iso_country_code", "currency_id"],
        ["maturitydate", "coupon", "mns_subclass"],
        ["maturitydate", "coupon"]
    ],

    "stocks": [
        ["iso_country_code", "currency_id", "mns_subclass"],
        ["iso_country_code", "currency_id"],
    ]

}


# Utility function to concatenate matched data
def concatenate_matches(bad_data, good_data, linked_records, save_domicile=False):

    # Build dataframe of matches
    match_bdf = bad_data[bad_data.index.isin([x[0][0] for x in linked_records])]
    match_gdf = good_data[good_data.index.isin([x[0][1] for x in linked_records])]
    link_ids = [(cluster_id, record_ids[0], record_ids[1], match_probability, match_round_number) for
                cluster_id, (record_ids, match_probability, match_round_number)
                in enumerate(linked_records)]
    ids = pd.DataFrame({
        'id_cluster': [x[0] for x in link_ids],
        'id_bdf': [x[1] for x in link_ids],
        'id_gdf': [x[2] for x in link_ids],
        'match_probability': [x[3] for x in link_ids],
        'match_round_number': [x[4] for x in link_ids],
    })

    # Merge dataframes
    match_bdf.index.name = 'id_bdf'
    match_bdf = match_bdf.reset_index().merge(ids[['id_cluster', 'id_bdf', 'match_probability', 'match_round_number']],
        how="left", on="id_bdf")
    match_bdf['provenance'] = 'bad_data'
    match_gdf.index.name = 'id_gdf'
    match_gdf = match_gdf.reset_index().merge(ids[['id_cluster', 'id_gdf', 'match_probability', 'match_round_number']],
        how="left", on="id_gdf")
    match_gdf['provenance'] = 'good_data'
    keep_cols = ['securityname', 'iso_country_code', 'currency_id', 'mns_subclass', 'id_cluster',
        'provenance', 'match_probability', 'match_round_number', 'coupon', 'maturitydate', 'geography',
        'extra_security_descriptors', 'securityname_raw']
    matches = pd.concat([match_bdf[keep_cols], match_gdf[keep_cols + ['cusip']]]).sort_values(['id_cluster', 'provenance'])
    matches = matches.reset_index().drop('index', axis=1)

    # Flatten the results
    final_rows = {
        'Ucoupon': [],
        'coupon': [],
        'Ucurrency_id': [],
        'currency_id': [],
        'Ucusip': [],
        'id_cluster': [],
        'Uiso_country_code': [],
        'iso_country_code': [],
        'match_probability': [],
        'match_round_number': [],
        'Umaturitydate': [],
        'maturitydate': [],
        'Umns_subclass': [],
        'mns_subclass': [],
        'Usecurityname': [],
        'securityname': [],
        'Usecurityname_raw': [],
        'securityname_raw': [],
        'extra_security_descriptors': []
    }
    if save_domicile:
        final_rows['Ugeography'] = []

    for i, gdf in matches.groupby("id_cluster"):

        gdata = gdf[gdf.provenance == 'good_data']
        bdata = gdf[gdf.provenance == 'bad_data']

        final_rows['Ucoupon'].append(gdata.coupon.values[0])
        final_rows['coupon'].append(bdata.coupon.values[0])

        final_rows['Ucurrency_id'].append(gdata.currency_id.values[0])
        final_rows['currency_id'].append(bdata.currency_id.values[0])

        final_rows['Ucusip'].append(gdata.cusip.values[0])
        final_rows['id_cluster'].append(gdata.id_cluster.values[0])
        final_rows['match_probability'].append(gdata.match_probability.values[0])
        final_rows['match_round_number'].append(gdata.match_round_number.values[0])
        final_rows['extra_security_descriptors'].append(gdata.extra_security_descriptors.values[0])

        final_rows['Uiso_country_code'].append(gdata.iso_country_code.values[0])
        final_rows['iso_country_code'].append(bdata.iso_country_code.values[0])

        final_rows['Umaturitydate'].append(gdata.maturitydate.values[0])
        final_rows['maturitydate'].append(bdata.maturitydate.values[0])

        final_rows['Umns_subclass'].append(gdata.mns_subclass.values[0])
        final_rows['mns_subclass'].append(bdata.mns_subclass.values[0])

        final_rows['Usecurityname'].append(gdata.securityname.values[0])
        final_rows['securityname'].append(bdata.securityname.values[0])

        final_rows['Usecurityname_raw'].append(gdata.securityname_raw.values[0])
        final_rows['securityname_raw'].append(bdata.securityname_raw.values[0])

        if save_domicile:
            final_rows['Ugeography'].append(gdata.geography.values[0])

    return pd.DataFrame(final_rows)


# Utility to run the logistic model for the prepass
def find_prepass_matches(group, fields_list, linker, round_number=0):
    """
    Finds matches in the prepass stage using the same logistic regression model
    as the main matching procedure.
    """
    # Get record sets
    linker_fields = [x.field for x in linker.data_model.primary_fields]
    gdata_cols = [x + "_u" for x in set(linker_fields)
            - set(fields_list)] + ['good_data_id']
    bdata_cols = linker_fields + ['bad_data_id']
    bdata = parse_missing_fields(df_to_dict(group[bdata_cols].head(1)))
    gdata_df = group[fields_list + gdata_cols].rename(columns={x:x.replace("_u", "") for x in gdata_cols})
    gdata_df.index += 1
    gdata = parse_missing_fields(df_to_dict(gdata_df))
    bpoint = bdata[bdata.keys()[0]]
    g_ids, gpoints = gdata.keys(), gdata.values()

    # Compute record distances
    distances = linker.data_model.distances([(bpoint, gpoint) for gpoint in gpoints])

    # Compute match probabilities
    match_probabilities = linker.classifier.predict_proba(distances)

    # Return matches, if any
    if np.max(match_probabilities) < acceptance_threshold:
        return []
    else:
        best_match_index = g_ids[np.argmax(match_probabilities)]
        best_match = gdata[best_match_index]["good_data_id"]
        return [((bpoint["bad_data_id"], best_match), np.max(match_probabilities), round_number)]


# Utility to construct matches; make call to logistic model
def run_prepass(fields_list, previous_matches, round_number, bad_data, good_data, linker):

    # Get indices of previous matches
    if len(previous_matches) > 0:
        matched_good_data = [x[0][1] for x in previous_matches]
        matched_bad_data = [x[0][0] for x in previous_matches]
    else:
        matched_bad_data, matched_good_data = [], []

    # Construct the hard matches
    hard_matches = bad_data[~bad_data.bad_data_id.isin(matched_bad_data)].dropna(subset=fields_list).merge(
        good_data[~good_data.good_data_id.isin(matched_good_data)].dropna(subset=fields_list),
        how="left",
        suffixes=("","_u"),
        on=fields_list
    ).dropna(subset=["good_data_id"])
    hard_matches.good_data_id = hard_matches.good_data_id.astype(int)
    hard_matches.bad_data_id = hard_matches.bad_data_id.astype(int)

    # Run the logistic model
    groups = hard_matches.groupby("bad_data_id")
    prepass_links = []
    for b_id, group in groups:
        group_links = find_prepass_matches(group, fields_list, linker, round_number)
        prepass_links.extend(group_links)
    return prepass_links


# Utility to run the logistic model for the prepass
def find_fullpass_matches(blocks):
    """
    Finds matches in the full-pass stage.
    """
    fullpass_matches = []
    fullpass_round_number = len(prepass_rounds)

    for i, block in enumerate(blocks):

        # Unpack the block
        bdata, gdata = block
        assert len(bdata) == 1, "Multiple bad datapoints in block. Did something go wrong in blocking?"
        bdata_id, bpoint, _ = bdata[0]
        gdata = {record[0]: record[1] for record in gdata}
        g_ids, gpoints = gdata.keys(), gdata.values()

        # Compute record distances
        distances = linker.data_model.distances([(bpoint, gpoint) for gpoint in gpoints])

        # Compute match probabilities
        match_probabilities = linker.classifier.predict_proba(distances)

        # Return matches, if any
        if np.max(match_probabilities) < acceptance_threshold:
            fullpass_matches.extend([])
        else:
            best_match = g_ids[np.argmax(match_probabilities)]
            fullpass_matches.extend([((bdata_id, best_match), np.max(match_probabilities), fullpass_round_number)])

        # Garbage collection and reporting
        if i % 10 == 0:
            logger.info("Processing block {}".format(i))
        if i % 200 == 0:
            gc.collect()

    return fullpass_matches


# Multiprocessing mapper with progress bar
def imap_unordered_bar(func, args, n_processes = 2):
    logger.info("Starting multiprocessing pool with {} cores".format(num_cores))
    p = Pool(n_processes)
    res_list = []
    with tqdm(total = len(args)) as pbar:
        for i, res in tqdm(enumerate(p.imap_unordered(func, args))):
            pbar.update()
            res_list.append(res)
    pbar.close()
    p.close()
    p.join()
    return res_list


# Main routine
if __name__ == "__main__":

    # Parse command line arguments
    parser = argparse.ArgumentParser()
    parser.add_argument("-p", "--partition", type=int, help="Array partition number")
    parser.add_argument("-a", "--assetclass", type=str, help="Asset class (bonds|stocks)")
    parser.add_argument("-g", "--geography", type=str, help="Geography (us|nonus|all)")
    parser.add_argument("-c", "--cpus", type=str, help="Geography (us|nonus|all)")
    parser.add_argument("-sd", "--scratchdir", type=str, help="Temporary data directory")
    parser.add_argument("-o", "--outputdir", type=str, help="Permanent output directory")
    parser.add_argument("-v", "--linkerversion", type=str, help="Linker version")
    parser.add_argument("-f", "--fullpass", type=int, help="Whether to run full pass (i.e., attempt to match items that don't hard-match along any dimension")
    parser.add_argument("-b", "--simplefullpass", type=int, help="Boolean flag; replaces full pass predicates with a simple fuzzy predicate on securityname")
    parser.add_argument("-r", "--rmprevious", type=float, help="Whether to skip previously matched items (boolean flag)")

    # Unpack the command line arguments
    args = parser.parse_args()
    asset_class = args.assetclass
    partition = args.partition
    domicile = args.geography
    num_cores = int(args.cpus)
    scratch_dir = args.scratchdir
    output_dir = args.outputdir
    linker_version = args.linkerversion
    do_full_pass = bool(args.fullpass)
    simple_full_pass = bool(args.simplefullpass)
    rm_previous = bool(args.rmprevious)
    mns_data_path = output_dir.replace("/output", "")

    # Set up logging
    logger = logging.getLogger(__name__)
    logfile = '{}/results/logs/{}_Fuzzy_Merge_Find_Matches_{}_{}_{}.log'.format(mns_data_path, getpass.getuser(), asset_class, domicile, partition)
    if os.path.exists(logfile):
        os.remove(logfile)
    logger.addHandler(logging.FileHandler(logfile))
    logger.addHandler(logging.StreamHandler(sys.stdout))
    sys.stderr = open(logfile, 'a')
    sys.stdout = open(logfile, 'a')

    # Some sanity checking
    assert asset_class in ["bonds", "stocks"], "Invalid asset class (must be in [bonds, stocks])"
    assert domicile in ["us", "nonus", "all"], "Invalid geography (must be in [us, nonus, all])"
    logger.info("Partition {} beginning deserialization".format(partition))
    logger.info("Using {} cores".format(num_cores))

    # Filter out previous matches as needed
    match_ids_path = "{}/match_ids_{}_v{}".format(scratch_dir, asset_class, linker_version)
    previous_matches = []
    if rm_previous:
        if not os.path.exists(match_ids_path):
            raise Exception("Asked to filter out previous matches but cannot find relevant records.")
        for fname in os.listdir(match_ids_path):
            with open(os.path.join(match_ids_path, fname), "rb") as f:
                matches_shard = cloudpickle.load(f)
            previous_matches += matches_shard

    # Deserialize the linker
    global linker
    linker = retrieve_linker(scratch_dir, asset_class, domicile, linker_version)
    linker.num_cores = num_cores
    logger.debug("Data linkage schema:")
    logger.debug(linker.data_model.primary_fields)

    # Set threshold
    global acceptance_threshold
    acceptance_threshold = linker.opt_threshold
    logger.info("Acceptance threshold = {}".format(acceptance_threshold))

    # Read in the bad data partition
    logger.info("Reading in bad data partition")
    bad_data_partition_path = "{}/bad_data_partitioned_{}_{}".format(scratch_dir, asset_class, domicile)
    bad_data_chunk = safe_deserialize("{}/bad_data_partition_{}.pkl".format(bad_data_partition_path, partition))

    # Read in the good data
    logger.info("Reading in good data")
    (_, good_data_dict) = safe_deserialize("{}/fuzzy_parsed_data_dicts_{}_{}.pkl".format(scratch_dir,
        asset_class, domicile))

    # Construct dataframe-shaped records for hard match
    logger.info("Constructing records for hard matches")
    bad_data_df = pd.DataFrame(bad_data_chunk).T
    good_data_df = pd.DataFrame(good_data_dict).T
    bad_data_df = bad_data_df.reset_index().rename(columns={'index': 'bad_data_id'})
    good_data_df = good_data_df.reset_index().rename(columns={'index': 'good_data_id'})

    # Simplify full pass if requested
    if simple_full_pass:
        predicates = [StringPredicate(commonSixGram, 'securityname')]
        linker.blocker = Blocker(predicates)

    # Some more memory savings
    logger.info("Clearing memory")
    for k in good_data_dict.keys():
        good_data_dict[k].pop('securityname_raw')
    for k in bad_data_chunk.keys():
        bad_data_chunk[k].pop('securityname_raw')

    # Run the multiple rounds of prepass matching, iteratively deleting previous matches
    prepass_matches = []
    for i, round_fields in enumerate(prepass_rounds[asset_class]):
        logger.info("Running prepass round {}".format(i))
        current_round_matches = run_prepass(round_fields, prepass_matches + previous_matches,
            round_number=i, bad_data=bad_data_df, good_data=good_data_df, linker=linker)
        prepass_matches.extend(current_round_matches)

    # Some reporting
    logger.info("Using prepass threshold {}, found a total of {} matches out of {} bad datapoints in shard".format(
        acceptance_threshold, len(prepass_matches), len(bad_data_chunk)
    ))

    # Get indices of previous matches
    matched_good_data = [x[0][1] for x in prepass_matches + previous_matches]
    matched_bad_data = [x[0][0] for x in prepass_matches + previous_matches]

    # Run the matching process on the remaining unmatched data
    if do_full_pass:

        # Clear up some memory
        logger.info('Matching records post-prepass; partition number = {}'.format(partition))
        del bad_data_df
        del good_data_df
        gc.collect()

        # Run the blocker
        bad_data_chunk_unmatched = {k:v for k,v in bad_data_chunk.items() if k not in matched_bad_data}
        good_data_dict_unmatched = {k:v for k,v in good_data_dict.items() if k not in matched_good_data}
        logger.info("Running the blocker")
        blocks = linker._blockData(bad_data_chunk_unmatched, good_data_dict_unmatched)

        # Run the linker
        logger.info("Running the linker")
        if num_cores > 1:
            block_partitions = list(get_list_chunks_fixedsize(blocks, 1))
            fullpass_matches = imap_unordered_bar(find_fullpass_matches, block_partitions, num_cores)
        else:
            fullpass_matches = [find_fullpass_matches(blocks)]
    else:
        fullpass_matches = []

    # Flatten the output
    if len(fullpass_matches) > 0:
        flatten = lambda x, y: x + y
        fullpass_matches = reduce(flatten, fullpass_matches)
    logger.info('Number of duplicate sets, post-prepass = {}'.format(len(fullpass_matches)))

    # Read in the standardized dataframes
    (bad_data, good_data) = safe_deserialize("{}/fuzzy_raw_data_standardized_{}_{}.pkl".format(scratch_dir,
        asset_class, domicile))

    # Concatenate the matches
    matches = concatenate_matches(bad_data, good_data, prepass_matches + fullpass_matches,
        save_domicile = True if domicile == "all" else False)

    # Save the matches
    match_partition_path = "{}/matches_partitioned_{}_{}_v{}".format(scratch_dir,
        asset_class, domicile, linker_version)
    if not os.path.exists(match_partition_path):
        os.makedirs(match_partition_path)
    matches.to_hdf("{}/matches_{}.h5".format(match_partition_path, partition), key="data")

    # Save the match IDs
    if not os.path.exists(match_ids_path):
        os.makedirs(match_ids_path)
    with open("{}/match_ids_{}_{}.pkl".format(match_ids_path, domicile, partition), "wb") as f:
        cloudpickle.dump(prepass_matches + fullpass_matches, f)

    # Close logs
    sys.stderr.close()
    sys.stdout.close()
    sys.stderr = sys.__stderr__
    sys.stdout = sys.__stdout__
