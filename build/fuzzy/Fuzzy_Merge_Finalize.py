# --------------------------------------------------------------------------------------------------
# Fuzzy_Merge_Finalize
#
# All files in this folder (fuzzy) handle the probabilistic record linkage of observations in the 
# Morningstar holdings data for which we lack a CUSIP identifier to other observations for which we
# do have an identifier. This allows us to assign a CUSIP to the former records via internal
# cross-linkage.
#
# This file collects the shards of matched data generated by the probabilistic record linker
# in the distributed Fuzzy_Merge_Find_Matches jobs, concatenates them, and stores them to disk.
# --------------------------------------------------------------------------------------------------
from __future__ import print_function
import os
import pandas as pd
import numpy as np
import argparse
import glob
import shutil
import logging
import getpass
import sys

# Hardwired settings (useful for debugging/development)
linker_version = 0

# Concatenation function
def concatenate_shards(match_partition_path):

    # List shards
    shards = os.listdir(match_partition_path)
    logger.info("Concatenating shards from {}".format(match_partition_path))
    logger.info("Found a total of {} shards".format(len(shards)))

    # Run the concatenation
    cluster_cursor = 0
    matches = pd.DataFrame({})
    for shard in shards:
        current_shard = pd.read_hdf("{}/{}".format(match_partition_path, shard))
        current_shard["id_cluster"] += cluster_cursor + 1
        matches = pd.concat([matches, current_shard]).sort_values(["match_probability", "id_cluster"],
                                                                  ascending=False)
        cluster_cursor = np.max(matches["id_cluster"])
    logger.info("There are a total of {} matches".format(cluster_cursor))
    return matches


if __name__ == "__main__":

    # Parse command line arguments
    parser = argparse.ArgumentParser()
    parser.add_argument("-a", "--assetclass", type=str, help="Asset class (bonds/stocks)")
    parser.add_argument("-sd", "--scratchdir", type=str, help="Temporary data directory")
    parser.add_argument("-o", "--outputdir", type=str, help="Permanent output directory")
    args = parser.parse_args()
    asset_class = args.assetclass
    scratch_dir = args.scratchdir
    output_dir = args.outputdir
    mns_data_path = output_dir.replace("/output", "")
    assert asset_class in ["bonds", "stocks"], "Invalid asset class (must be in [bonds, stocks])"

    # Set up logging
    logger = logging.getLogger(__name__)
    logfile = '{}/results/logs/{}_Fuzzy_Merge_Finalize_{}.log'.format(mns_data_path, getpass.getuser(), asset_class)
    if os.path.exists(logfile):
        os.remove(logfile)
    logger.addHandler(logging.FileHandler(logfile))
    logger.addHandler(logging.StreamHandler(sys.stdout))
    sys.stderr = open(logfile, 'a')
    sys.stdout = open(logfile, 'a')
    logger.info("Finalizing matches for {}".format(asset_class))

    # Get the matches; store to disk
    if asset_class == "stocks":

        # Split US and NonUS matches for downstream processing
        match_partition_path = "{}/matches_partitioned_{}_{}_v{}".format(scratch_dir, asset_class, "all", linker_version)
        matches = concatenate_shards(match_partition_path)
        matches[matches.Ugeography == "US"].to_stata(os.path.join(output_dir, "fuzzy", "fuzzy_matches_{}_{}.dta".format(asset_class, "US")))
        matches[matches.Ugeography == "NonUS"].to_stata(os.path.join(output_dir, "fuzzy", "fuzzy_matches_{}_{}.dta".format(asset_class, "NonUS")))

    elif asset_class == "bonds":

        # Augment data with residual matches
        matches = {}
        for domicile in ['us', 'nonus']:
            match_partition_path = "{}/matches_partitioned_{}_{}_v{}".format(scratch_dir, asset_class, domicile, linker_version)
            matches[domicile] = concatenate_shards(match_partition_path)
        residual_match_partition_path = "{}/matches_partitioned_{}_{}_v{}".format(scratch_dir, asset_class, "all", linker_version)
        residual_matches = concatenate_shards(residual_match_partition_path)
        matches['us'] = pd.concat([matches['us'], residual_matches[residual_matches.Ugeography == "US"]])
        matches['nonus'] = pd.concat([matches['us'], residual_matches[residual_matches.Ugeography == "NonUS"]])

        # Cast types
        for dfield in ['maturitydate', 'Umaturitydate']:
            matches['us'][dfield] = pd.to_datetime(matches['us'][dfield])
            matches['nonus'][dfield] = pd.to_datetime(matches['nonus'][dfield])
        for sfield in ['Ucurrency_id', 'Ucusip', 'Ugeography', 'Uiso_country_code',
         'Umns_subclass', 'Usecurityname', 'Usecurityname_raw', 'currency_id',
         'extra_security_descriptors', 'iso_country_code', 'mns_subclass',
         'securityname','securityname_raw']:
            matches['us'][sfield] = matches['us'][sfield].astype(str)
            matches['nonus'][sfield] = matches['nonus'][sfield].astype(str)

        # Save output
        matches["us"].to_stata(os.path.join(output_dir, "fuzzy", "fuzzy_matches_{}_{}.dta".format(asset_class, "US")))
        matches["nonus"].to_stata(os.path.join(output_dir, "fuzzy", "fuzzy_matches_{}_{}.dta".format(asset_class, "NonUS")))

    # Finally clean up all the intermediate files
    for f in glob.glob("{}/fuzzy_parsed_data_dicts_{}*".format(scratch_dir, asset_class)):
        os.remove(f)
    for f in glob.glob("{}/fuzzy_raw_data_standardized_{}*".format(scratch_dir, asset_class)):
        os.remove(f)
    for f in glob.glob("{}/serialized_linker_{}*".format(scratch_dir, asset_class)):
        os.remove(f)
    dirs = []
    dirs = dirs + ["{}/bad_data_partitioned_{}_{}".format(scratch_dir, asset_class, domicile) for domicile in ['us', 'nonus', 'all']]
    dirs = dirs + ["{}/matches_partitioned_{}_{}_v{}".format(scratch_dir, asset_class, domicile, linker_version) for domicile in ['us', 'nonus', 'all']]
    dirs = dirs + ["{}/match_ids_{}_{}_v{}".format(scratch_dir, asset_class, domicile, linker_version) for domicile in ['us', 'nonus', 'all']]
    for dir_p in dirs:
        if os.path.exists(dir_p):
            shutil.rmtree(dir_p)

    # Close logs
    sys.stderr.close()
    sys.stdout.close()
    sys.stderr = sys.__stderr__
    sys.stdout = sys.__stdout__
